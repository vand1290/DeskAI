# DeskAI OCR â€” Working Plan to MVP

## Overview

DeskAI now includes an **offline-first OCR system** for extracting text from:
- **Images** (PNG, JPG, TIFF, BMP)
- **PDFs** (via image rendering)
- **Handwritten text** (via PARSeq model)
- **Printed documents** (via TrOCR model)

Supports **multiple languages**: English, Romanian, Italian (extensible).

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Document â”‚ (Image / PDF)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PREPROCESSING                              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ â€¢ Deskew (Hough lines)                      â”‚
    â”‚ â€¢ Invert detect (mean pixel analysis)       â”‚
    â”‚ â€¢ Adaptive binarization                     â”‚
    â”‚ â€¢ Light dewarp (polynomial warping)         â”‚
    â”‚ â€¢ Normalize to height=32px, width=512px     â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ DETECTION         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ CRAFT / DBNet     â”‚
    â”‚ (text regions)    â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ RECOGNITION (per line)                 â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ TrOCR (seq2seq) + PARSeq (CTC)         â”‚
    â”‚ Both run in parallel                    â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ROUTING (confidence-based merge)        â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ If TrOCR conf â‰¥ 0.7: use TrOCR        â”‚
    â”‚ Else: boost PARSeq (+0.1) and pick bestâ”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ POSTPROCESSING                           â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ â€¢ Normalize dates (YYYY-MM-DD)           â”‚
    â”‚ â€¢ Normalize amounts ($1,234.56 â†’ 1234.56)â”‚
    â”‚ â€¢ Extract totals (regex patterns)        â”‚
    â”‚ â€¢ Language guessing (en/ro/it)           â”‚
    â”‚ â€¢ Currency mapping                       â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ OUTPUT (JSON + Database)    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ {                           â”‚
    â”‚   "text": "...",            â”‚
    â”‚   "confidence": 0.85,       â”‚
    â”‚   "boxes": [...],           â”‚
    â”‚   "language": "en",         â”‚
    â”‚   "totals": {...}           â”‚
    â”‚ }                           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Project Structure

```
ocr/
â”œâ”€â”€ __init__.py                 # Package init
â”œâ”€â”€ preprocess.py              # Preprocessing pipeline
â”œâ”€â”€ detect.py                  # Text detection (CRAFT/DBNet)
â”œâ”€â”€ recognize.py               # Recognition (TrOCR/PARSeq)
â”œâ”€â”€ router.py                  # Model routing + postprocessing
â”œâ”€â”€ cli.py                     # Command-line interface
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ config.yaml                # Configuration template
â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py             # CER/WER evaluation
â”‚   â””â”€â”€ expected_manifest.json # Ground truth for bad-cases
â””â”€â”€ bad_cases/                 # Sample problematic documents
    â”œâ”€â”€ bad_case_1.png         # (placeholder)
    â”œâ”€â”€ bad_case_2.png
    â””â”€â”€ ...

02_models_data/               # ONNX model weights (download here)
â”œâ”€â”€ trocr_base.onnx           # ~700MB
â”œâ”€â”€ parseq.onnx                # ~100MB
â”œâ”€â”€ dbnet.onnx                 # ~200MB (optional)
â””â”€â”€ craft.onnx                 # ~200MB (optional)
```

---

## Quick Start

### 1. Setup Environment

```powershell
# Windows PowerShell
cd c:\Users\ACESFG167279MF\Desktop\DocBrain\docbrain-starter

# Create virtual environment
python -m venv .venv
. .\.venv\Scripts\Activate.ps1

# Install OCR dependencies
pip install -r ocr/requirements.txt
```

### 2. Download Models

Models are large (~1-1.5GB total) and must be downloaded separately:

```bash
# Download TrOCR (seq2seq for printed text)
# From: https://huggingface.co/microsoft/trocr-base-printed
# Save to: 02_models_data/trocr_base.onnx

# Download PARSeq (CTC for handwriting)
# From: https://huggingface.co/baudm/parseq
# Save to: 02_models_data/parseq.onnx
```

For now, the system runs with **placeholder models** (returns mock data) to test the pipeline.

### 3. Process a Document

```powershell
# Test image
python -m ocr.cli path/to/invoice.png -o out.json

# Test PDF (@220 DPI)
python -m ocr.cli path/to/document.pdf -o out.json --dpi 220

# Verbose output
python -m ocr.cli path/to/image.png -o out.json -v

# Custom config
python -m ocr.cli path/to/image.png -o out.json --config ocr/config.yaml
```

### 4. Evaluate Results

```powershell
# Run evaluation against expected manifest
python -m ocr.eval.metrics \
  --manifest ocr/eval/expected_manifest.json \
  --pred out.json
```

---

## Configuration (config.yaml)

Key tuning parameters:

```yaml
preprocess:
  binarize_method: adaptive    # Thresholding method
  invert_threshold_mean: 127   # When to invert (dark text)
  height_px: 32                # Line height
  gaussian_blur: 1             # Smoothing

router:
  trocr_min_conf: 0.7          # Confidence threshold for TrOCR
  parseq_bias: 0.1             # Bonus for PARSeq (cursive)

postprocess:
  locale: en_US                # Language for normalization
  enable_language_guess: true  # Auto-detect language
```

---

## Integration with DeskAI Desktop App

### OCR Tab in UI

The desktop app has an **OCR** button that:

1. Opens file picker â†’ select image/PDF
2. Runs preprocessing + detection + recognition
3. Displays extracted text + confidence
4. Shows detected bounding boxes
5. Saves to database + JSON export

### Python API

```python
from ocr.cli import process_image
from ocr.router import Router, Postprocessor

# Process image
result = process_image("invoice.png")

# Result structure
{
    "image": "invoice.png",
    "text": "...",
    "confidence": 0.85,
    "lines": [
        {
            "text": "Line 1",
            "confidence": 0.90,
            "box": [[x,y], [x,y], ...],
            "model": "trocr",
            "language": "en"
        },
        ...
    ],
    "metadata": {
        "preprocessed": {...},
        "detector": "craft",
        "num_lines": 15,
        "inverted": false
    }
}
```

---

## KPI Targets (MVP)

| Metric | Target | Status |
|--------|--------|--------|
| **Line CER** | â‰¤ 8% (bad-cases 1/2/3/5) | ğŸ”„ Tuning |
| **Line CER** | â‰¤ 5% (bad-case 4 after inversion) | ğŸ”„ Tuning |
| **Field Accuracy** | Dates â‰¥ 98% | ğŸ”„ Testing |
| **Field Accuracy** | Amounts â‰¥ 98% | ğŸ”„ Testing |
| **Field Accuracy** | Totals â‰¥ 99% | ğŸ”„ Testing |
| **Latency (CPU)** | A4 @300DPI â‰¤ 6s | ğŸ”„ Benchmarking |
| **Latency (GPU)** | A4 @300DPI â‰¤ 1.5s | ğŸ”„ Benchmarking |

---

## Mitigations for Known Issues

### Issue: Handwriting Not Recognized
**Solution**: Route low-confidence TrOCR results to PARSeq with confidence boost.

### Issue: Glare / Mobile Photos
**Solution**: 
- Aggressive inversion detection
- Adaptive binarization
- Light perspective correction

### Issue: Multiple Locales (EN/RO/IT)
**Solution**:
- Language guessing module
- Locale-specific normalizers
- Multi-language training later

---

## Next Steps (5-Day Roadmap)

### Day 1â€“2: Model Integration
- [ ] Download ONNX models (TrOCR, PARSeq)
- [ ] Verify ONNXRuntime inference
- [ ] Test on sample images

### Day 3: Preprocessing Tuning
- [ ] Collect bad-cases
- [ ] Tune binarization / dewarp
- [ ] Measure CER improvements

### Day 4: Field Extraction & Routing
- [ ] Implement confidence router
- [ ] Add regex patterns for dates/totals
- [ ] Normalize currencies

### Day 5: MVP & Deployment
- [ ] End-to-end testing (image â†’ JSON)
- [ ] PDF multi-page support
- [ ] Package as .exe (PyInstaller)

### 90-Day Roadmap (Post-MVP)
1. **Model Fine-tuning** (Week 2â€“3)
   - Collect invoice/receipt dataset
   - Fine-tune TrOCR + PARSeq on domain data

2. **Performance Optimization** (Week 4)
   - CUDA / DirectML GPU acceleration
   - Model quantization (int8)
   - Batch inference

3. **Advanced Features** (Week 5â€“8)
   - Page-shape detection (perspective correction)
   - Table structure recognition
   - Handwriting segmentation

4. **Scale & Polish** (Week 9â€“12)
   - Production database integration
   - API layer (REST)
   - Web UI for batch processing

---

## Testing

### Unit Tests

```powershell
pytest ocr/ -v
pytest ocr/ -v --cov=ocr --cov-report=html
```

### Manual Testing

```powershell
# Test preprocessing
python -c "from ocr.preprocess import Preprocessor; p = Preprocessor(); print(p.config.__dict__)"

# Test detection
python -c "from ocr.detect import Detector; d = Detector(); print(d.detect_craft(...))"

# Test recognition
python -c "from ocr.recognize import Recognizer; r = Recognizer(); print(r.recognize(...))"
```

---

## References

- **TrOCR Paper**: https://arxiv.org/abs/2109.10282
- **PARSeq Paper**: https://arxiv.org/abs/2207.06966
- **CRAFT Detection**: https://arxiv.org/abs/1904.01941
- **ONNXRuntime**: https://onnxruntime.ai/
- **Hugging Face Models**: https://huggingface.co/

---

## Support

For questions:
- Check `ocr/config.yaml` for tuning
- Run with `-v` flag for debug logs
- Review `ocr/eval/metrics.py` for evaluation details

**Status**: âœ… **Ready for MVP integration** (placeholder models in place, full models to be downloaded)
